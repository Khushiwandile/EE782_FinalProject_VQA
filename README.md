# EE782_FinalProject_VQA


---

##  Pipeline Phases

### **Phase 1 â€” Dataset Preprocessing**
- Loaded VQA v2 dataset  
- Tokenized questions using BERT  
- Generated soft-label answer vectors (3000-class)  
- Built `processed_samples` list  

### **Phase 2 â€” Baseline VisualBERT**
- Implemented a simplified VisualBERT  
- Verified training pipeline using dummy visual features  

### **Phase 3 â€” CLIP Vision Patch Extraction**
- Loaded CLIP ViT-B/32  
- Extracted **49Ã—768 patch tokens** per image  
- Cached features for fast training  

### **Phase 4 â€” CLIP-VisualBERT Fusion Model**
- Added:
  - CLIP â†’ BERT projection MLP  
  - Positional embeddings  
  - Modality embeddings  
  - Fusion via BERT encoder  

### **Phase 5 â€” Training & Saving**
- BCE loss with VQA soft targets  
- AdamW optimizer  
- Saved checkpoint:

  ### **Phase 8 â€” Gradio App**
Interactive web demo where users upload an image and ask any question.

---

##  Results

| Model | Accuracy |
|-------|----------|
| VisualBERT Baseline | ~0.20 |
| **CLIP-VisualBERT (ours)** | **0.34â€“0.36** |

---

## ðŸ›  Installation

```bash
pip install torch torchvision transformers gradio pillow tqdm
pip install git+https://github.com/openai/CLIP.git
